{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from catboost import CatBoostRegressor\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "from subprocess import check_output\n",
    "print(check_output([\"ls\", \"./input\"]).decode(\"utf8\"))\n",
    "\n",
    "train_df = pd.read_csv('./input/train_2016_v2.csv', parse_dates=['transactiondate'], low_memory=False)\n",
    "test_df = pd.read_csv('./input/sample_submission.csv', low_memory=False)\n",
    "properties = pd.read_csv('./input/properties_2016.csv', low_memory=False)\n",
    "# field is named differently in submission\n",
    "test_df['parcelid'] = test_df['ParcelId']\n",
    "\n",
    "# similar to the1owl\n",
    "def add_date_features(df):\n",
    "    df[\"transaction_year\"] = df[\"transactiondate\"].dt.year\n",
    "    df[\"transaction_month\"] = df[\"transactiondate\"].dt.month\n",
    "    df[\"transaction_day\"] = df[\"transactiondate\"].dt.day\n",
    "    df[\"transaction_quarter\"] = df[\"transactiondate\"].dt.quarter\n",
    "    df.drop([\"transactiondate\"], inplace=True, axis=1)\n",
    "    return df\n",
    "train_df = add_date_features(train_df)\n",
    "#test_df = add_date_features(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def add_pro_features(prop_df):\n",
    "    zip_count = prop_df['regionidzip'].value_counts().to_dict()\n",
    "    city_count = prop_df['regionidcity'].value_counts().to_dict()\n",
    "    prop_df['N-zip_count'] = prop_df['regionidzip'].map(zip_count)\n",
    "    prop_df['N-city_count'] = prop_df['regionidcity'].map(city_count)\n",
    "    prop_df['N-GarPoolAC'] = ((prop_df['garagecarcnt']>0) & \n",
    "    (prop_df['pooltypeid10']>0) & (prop_df['airconditioningtypeid']!=5))*1\n",
    "    \n",
    "    prop_df['N-ValueRatio'] = prop_df['taxvaluedollarcnt']/prop_df['taxamount']\n",
    "    prop_df['N-LivingAreaProp'] = prop_df['calculatedfinishedsquarefeet']/prop_df['lotsizesquarefeet']\n",
    "    prop_df['N-ValueProp'] = prop_df['structuretaxvaluedollarcnt']/prop_df['landtaxvaluedollarcnt']\n",
    "    group = prop_df.groupby('regionidcity')['structuretaxvaluedollarcnt'].aggregate('mean').to_dict()\n",
    "    prop_df['N-Avg-structuretaxvaluedollarcnt'] = prop_df['regionidcity'].map(group)\n",
    "\n",
    "    #Deviation away from average\n",
    "\n",
    "    prop_df['N-Dev-structuretaxvaluedollarcnt'] = abs((prop_df['structuretaxvaluedollarcnt'] - prop_df['N-Avg-structuretaxvaluedollarcnt']))/prop_df['N-Avg-structuretaxvaluedollarcnt']\n",
    "    prop_df['N-TaxScore'] = prop_df['taxvaluedollarcnt']*prop_df['taxamount']\n",
    "\n",
    "    #Number of properties in the zip\n",
    "    zip_count = prop_df['regionidzip'].value_counts().to_dict()\n",
    "    prop_df['N-zip_count'] = prop_df['regionidzip'].map(zip_count)\n",
    "\n",
    "    #Number of properties in the city\n",
    "    city_count = prop_df['regionidcity'].value_counts().to_dict()\n",
    "    prop_df['N-city_count'] = prop_df['regionidcity'].map(city_count)\n",
    "\n",
    "    #Number of properties in the city\n",
    "    region_count = prop_df['regionidcounty'].value_counts().to_dict()\n",
    "    prop_df['N-county_count'] = prop_df['regionidcounty'].map(city_count)\n",
    "\n",
    "\n",
    "    return prop_df\n",
    "\n",
    "properties = add_pro_features(properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df = train_df.merge(properties, how='left', on='parcelid')\n",
    "# train_df= train_df[ train_df.logerror > -0.4]\n",
    "# train_df= train_df[ train_df.logerror < 0.419]\n",
    "test_df = test_df.merge(properties, how='left', on='parcelid')\n",
    "print(\"Train: \", train_df.shape)\n",
    "print(\"Test: \", test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "missing_perc_thresh = 0.98\n",
    "exclude_missing = []\n",
    "num_rows = train_df.shape[0]\n",
    "for c in train_df.columns:\n",
    "    num_missing = train_df[c].isnull().sum()\n",
    "    if num_missing == 0:\n",
    "        continue\n",
    "    missing_frac = num_missing / float(num_rows)\n",
    "    if missing_frac > missing_perc_thresh:\n",
    "        exclude_missing.append(c)\n",
    "print(\"We exclude: %s\" % exclude_missing)\n",
    "print(len(exclude_missing))\n",
    "\n",
    "# exclude where we only have one unique value :D\n",
    "exclude_unique = []\n",
    "for c in train_df.columns:\n",
    "    num_uniques = len(train_df[c].unique())\n",
    "    if train_df[c].isnull().sum() != 0:\n",
    "        num_uniques -= 1\n",
    "    if num_uniques == 1:\n",
    "        exclude_unique.append(c)\n",
    "print(\"We exclude: %s\" % exclude_unique)\n",
    "print(len(exclude_unique))\n",
    "\n",
    "exclude_other = ['parcelid', 'logerror']  # for indexing/training only\n",
    "# do not know what this is LARS, 'SHCG' 'COR2YY' 'LNR2RPD-R3' ?!?\n",
    "exclude_other.append('propertyzoningdesc')\n",
    "train_features = []\n",
    "for c in train_df.columns:\n",
    "    if c not in exclude_missing \\\n",
    "       and c not in exclude_other and c not in exclude_unique:\n",
    "        train_features.append(c)\n",
    "print(\"We use these for training: %s\" % train_features)\n",
    "print(len(train_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cat_feature_inds = []\n",
    "cat_unique_thresh = 1000\n",
    "for i, c in enumerate(train_features):\n",
    "    num_uniques = len(train_df[c].unique())\n",
    "    if num_uniques < cat_unique_thresh \\\n",
    "       and not 'sqft' in c \\\n",
    "       and not 'cnt' in c \\\n",
    "       and not 'nbr' in c \\\n",
    "       and not 'number' in c:\n",
    "        cat_feature_inds.append(i)\n",
    "        \n",
    "print(\"Cat features are: %s\" % [train_features[ind] for ind in cat_feature_inds])\n",
    "\n",
    "# some out of range int is a good choice\n",
    "train_df.fillna(-999, inplace=True)\n",
    "test_df.fillna(-999, inplace=True)\n",
    "\n",
    "X_train = train_df[train_features]\n",
    "y_train = train_df.logerror\n",
    "\n",
    "\n",
    "sel_month = X_train.transaction_month >= 10\n",
    "# X_val = X_train[sel_month]\n",
    "# y_val = y_train[sel_month]\n",
    "# X_train = X_train[~sel_month]\n",
    "# y_train = y_train[~sel_month]\n",
    "\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from sortedcontainers import SortedList\n",
    "# import copy\n",
    "# import collections\n",
    "# import numpy as np\n",
    "# from itertools import product,chain\n",
    "# import pandas\n",
    "# from sklearn.model_selection import KFold\n",
    "# import catboost as cb\n",
    "\n",
    "# ''' a class for doing grid search on a set of parameters provided in a dict. 'pdict' should be a dictionary like the following:\n",
    "# pdict = {'depth':[1,2], 'iterations':[250,100,500], 'thread_count':4}\n",
    "\n",
    "# when grid_search is called it will return an iterator that provides samples from the dictionary e.g.\n",
    "# {'depth':1, 'iterations':250, 'thread_count':4}\n",
    "# {'depth':2, 'iterations':250, 'thread_count':4}\n",
    "# {'depth':1, 'iterations':100, 'thread_count':4}\n",
    "# etc.\n",
    "# after calling an iteration of grid_search, you need to test the classifier and run 'register_result'\n",
    "# This will update the internal list of results, so that the next call to grid_search will use the best\n",
    "# parameters for all the parameters not currently being updated.\n",
    "\n",
    "# grid_search can be provided a list e.g. grid_search(['depth']) this will use the current best parameters for all\n",
    "# the other arguments and only search over 'depth'. You can then call e.g. grid_search(['iterations']) and it will use\n",
    "# the best depth found previously and cycle through all the 'iterations'. Searching incrementally can be much faster\n",
    "# than doing a full grid search, but may miss the global optimum. '''\n",
    "# class Paramsearch:\n",
    "#     def __init__(self,pdict):    \n",
    "#         self.pdict = {}\n",
    "#         # if something is not passed in as a sequence, make it a sequence with 1 element\n",
    "#         #   don't treat strings as sequences\n",
    "#         for a,b in pdict.items():\n",
    "#             if isinstance(b, collections.Sequence) and not isinstance(b, str): self.pdict[a] = b\n",
    "#             else: self.pdict[a] = [b]\n",
    "#         # our results are a sorted list, so the best score is always the final element\n",
    "#         self.results = SortedList()       \n",
    "                    \n",
    "#     def grid_search(self,keys=None):\n",
    "#         # do grid search on only the keys listed. If none provided, do all\n",
    "#         if keys==None: keylist = self.pdict.keys()\n",
    "#         else: keylist = keys\n",
    " \n",
    "#         listoflists = [] # this will be list of lists of key,value pairs\n",
    "#         for key in keylist: listoflists.append([(key,i) for i in self.pdict[key]])\n",
    "#         for p in product(*listoflists):\n",
    "#             # do any changes to the current best parameter set\n",
    "#             if len(self.results)>0: template = self.results[-1][1]\n",
    "#             else: template = {a:b[0] for a,b in self.pdict.items()}\n",
    "#             # if our updates are the same as current best, don't bother\n",
    "#             if self.equaldict(dict(p),template): continue\n",
    "#             # take the current best and update just the ones to change\n",
    "#             yield self.overwritedict(dict(p),template)\n",
    "                              \n",
    "#     def equaldict(self,a,b):\n",
    "#         for key in a.keys(): \n",
    "#             if a[key] != b[key]: return False\n",
    "#         return True            \n",
    "                              \n",
    "#     def overwritedict(self,new,old):\n",
    "#         old = copy.deepcopy(old)\n",
    "#         for key in new.keys(): old[key] = new[key]\n",
    "#         return old            \n",
    "    \n",
    "#     # save a (score,params) pair to results. Since 'results' is a sorted list,\n",
    "#     #   the best score is always the final element. A small amount of noise is added\n",
    "#     #   because sorted lists don't like it when two scores are exactly the same    \n",
    "#     def register_result(self,result,params):\n",
    "#         self.results.add((result+np.random.randn()*1e-10,params))    \n",
    "        \n",
    "#     def bestscore(self):\n",
    "#         return self.results[-1][0]\n",
    "        \n",
    "#     def bestparam(self):\n",
    "#         return self.results[-1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import paramsearch\n",
    "# from sklearn import metrics\n",
    "# test_df['transactiondate'] = pd.Timestamp('2016-12-01')  # Dummy\n",
    "# test_df = add_date_features(test_df)\n",
    "# X_test = test_df[train_features]\n",
    "# print(X_test.shape)\n",
    "\n",
    "# num_ensembles = 5\n",
    "# y_pred = 0.0\n",
    "\n",
    "# params = {'depth':[5,6],\n",
    "#           'iterations':[250,500],\n",
    "#           'learning_rate':[0.01,0.03], \n",
    "#           'l2_leaf_reg':[3],\n",
    "#           'thread_count':4}\n",
    "\n",
    "# def crossvaltest(params,train_set,train_label,cat_dims,n_splits=3):\n",
    "#     kf = KFold(n_splits=n_splits,shuffle=True) \n",
    "#     res = []\n",
    "#     for train_index, test_index in kf.split(train_set):\n",
    "#         train = train_set.iloc[train_index,:]\n",
    "#         test = train_set.iloc[test_index,:]\n",
    "\n",
    "#         labels = train_label.ix[train_index]\n",
    "#         test_labels = train_label.ix[test_index]\n",
    "\n",
    "#         clf = cb.CatBoostRegressor(**params)\n",
    "#         clf.fit(train, labels, cat_features=cat_feature_inds)\n",
    "\n",
    "#         res.append((metrics.mean_absolute_error(clf.predict(test) , test_labels)))\n",
    "#     return np.mean(res)\n",
    "\n",
    "\n",
    "# # this function runs grid search on several parameters\n",
    "# def catboost_param_tune(params,train_set,train_label,cat_dims=None,n_splits=2):\n",
    "#     ps = Paramsearch(params)\n",
    "#     # search 'border_count', 'l2_leaf_reg' etc. individually \n",
    "#     #   but 'iterations','learning_rate' together\n",
    "#     for prms in chain(ps.grid_search(['l2_leaf_reg']),\n",
    "#                       ps.grid_search(['iterations','learning_rate']),\n",
    "#                       ps.grid_search(['depth'])):\n",
    "#         res = crossvaltest(prms,train_set,train_label,cat_dims,n_splits)\n",
    "#         # save the crossvalidation result so that future iterations can reuse the best parameters\n",
    "#         ps.register_result(res,prms)\n",
    "#         print(res,prms,'best:',ps.bestscore(),ps.bestparam())\n",
    "#     return ps.bestparam()\n",
    "\n",
    "# bestparams = catboost_param_tune(params,X_train,y_train,cat_feature_inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# bestparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bestparams = {'depth': 6,\n",
    " 'iterations': 500,\n",
    " 'l2_leaf_reg': 3,\n",
    " 'learning_rate': 0.03,\n",
    " 'thread_count': 4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# bestparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train classifier with tuned parameters    \n",
    "#clf = cb.CatBoostRegressor(**bestparams)\n",
    "#clf.fit(train_set, np.ravel(train_label), cat_features=None)\n",
    "#res = clf.predict(test_set)\n",
    "# print('error:',1-np.mean(res==np.ravel(test_label)))\n",
    "\n",
    "\n",
    "\n",
    "#for i in tqdm(range(num_ensembles)):\n",
    "# TODO(you): Use CV, tune hyperparameters\n",
    "model = CatBoostRegressor(\n",
    "     iterations=bestparams['iterations'], learning_rate=bestparams['learning_rate'],\n",
    "     depth=bestparams['depth'], l2_leaf_reg=bestparams['l2_leaf_reg'])\n",
    "#     loss_function='MAE',\n",
    "#     eval_metric='MAE',\n",
    "#     random_seed=i)\n",
    "# model = CatBoostRegressor(**params)\n",
    "model.fit(\n",
    "     X_train, y_train,\n",
    "     cat_features=cat_feature_inds)\n",
    "\n",
    "test_df['transactiondate'] = pd.Timestamp('2016-12-01')  # Dummy\n",
    "test_df = add_date_features(test_df)\n",
    "X_test = test_df[train_features]\n",
    "\n",
    "#     eval_set = (X_val, y_val))\n",
    "#     #y_pred += model.predict(X_val)\n",
    "y_pred = model.predict(X_test)\n",
    "# y_pred /= num_ensembles\n",
    "#from sklearn import metrics\n",
    "#y_pred = model.predict(X_val)\n",
    "#print(metrics.mean_absolute_error(y_pred , y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "    'ParcelId': test_df['parcelid'],\n",
    "})\n",
    "# https://www.kaggle.com/c/zillow-prize-1/discussion/33899, Oct,Nov,Dec\n",
    "test_dates = {\n",
    "    '201610': pd.Timestamp('2016-09-30'),\n",
    "    '201611': pd.Timestamp('2016-10-31'),\n",
    "    '201612': pd.Timestamp('2016-11-30'),\n",
    "    '201710': pd.Timestamp('2017-09-30'),\n",
    "    '201711': pd.Timestamp('2017-10-31'),\n",
    "    '201712': pd.Timestamp('2017-11-30')\n",
    "}\n",
    "for label, test_date in test_dates.items():\n",
    "    print(\"Predicting for: %s ... \" % (label))\n",
    "    # TODO(you): predict for every `test_date`\n",
    "    submission[label] = y_pred\n",
    "\n",
    "submission_major = 1\n",
    "submission.to_csv(\n",
    "    'submission_%03d.csv' % (submission_major),\n",
    "    float_format='%.4f',\n",
    "    index=False)\n",
    "print(\"Done! Good luck with submission #%d :)\" % submission_major)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/gaolingnan/assignment1/zillow\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import paramsearch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test_df['transactiondate'] = pd.Timestamp('2016-12-01')  # Dummy\n",
    "# test_df = add_date_features(test_df)\n",
    "# X_test = test_df[train_features]\n",
    "# print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
